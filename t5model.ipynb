{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_TOKEN\"] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 2.52MB/s]\n",
      "Downloading tokenizer_config.json: 100%|██████████| 2.32k/2.32k [00:00<00:00, 775kB/s]\n",
      "Downloading config.json: 100%|██████████| 1.21k/1.21k [00:00<00:00, 402kB/s]\n",
      "Downloading tf_model.h5: 100%|██████████| 242M/242M [00:18<00:00, 13.3MB/s] \n",
      "All model checkpoint layers were used when initializing TFT5Model.\n",
      "\n",
      "All the layers of TFT5Model were initialized from the model checkpoint at google-t5/t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5Model for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFViTModel.\n",
      "\n",
      "All the layers of TFViTModel were initialized from the model checkpoint at google/vit-base-patch16-224-in21k.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFViTModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "from transformers import T5Tokenizer, TFT5Model\n",
    "from transformers import ViTImageProcessor, TFViTModel\n",
    "\n",
    "pretrained_t5_path = 'google-t5/t5-small'\n",
    "pretrained_vit_path = 'google/vit-base-patch16-224-in21k'\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(pretrained_t5_path)\n",
    "t5 = TFT5Model.from_pretrained(pretrained_t5_path)\n",
    "t5_encoder = t5.encoder\n",
    "\n",
    "vit = TFViTModel.from_pretrained(pretrained_vit_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, TFT5Model, TFViTModel\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "import tensorflow as tf\n",
    "\n",
    "def VQAModel(t5):\n",
    "    visual_embedding_shape = (197, 768)\n",
    "    text_embedding_shape = (15, 512)\n",
    "    \n",
    "    visual_embedding = Input(visual_embedding_shape, name='visual_embedding')\n",
    "    text_embedding = Input(text_embedding_shape, name='text_embedding')\n",
    "\n",
    "    # 視覚埋め込みをテキスト埋め込みの次元に合わせて変換\n",
    "    x_v = layers.Dense(\n",
    "        units=text_embedding_shape[1], \n",
    "        activation='relu', \n",
    "        use_bias=True,\n",
    "    )(visual_embedding)\n",
    "    \n",
    "    # 注意機構を適用して視覚とテキストの埋め込みを組み合わせる\n",
    "    attention_output = layers.Attention()([x_v, text_embedding])\n",
    "    \n",
    "    # 出力を結合\n",
    "    x = layers.Concatenate(axis=1)([attention_output, text_embedding])\n",
    "\n",
    "    # 結合された埋め込みをT5デコーダに入力するためのエンコードを行う\n",
    "    decoder_input_ids = layers.Input(shape=(None,), dtype=tf.int32, name='decoder_input_ids')\n",
    "    t5_output = t5.decoder(input_ids=decoder_input_ids, encoder_hidden_states=x).last_hidden_state\n",
    "\n",
    "    return Model(inputs=[visual_embedding, text_embedding, decoder_input_ids], outputs=t5_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 3.1208003e-02  9.1505021e-02 -1.1999548e-01 ...  7.0121162e-02\n",
      "   -5.2835385e-05 -2.0129712e-01]\n",
      "  [ 2.2196151e-02  1.0012906e-01 -3.8860645e-02 ...  3.2922305e-02\n",
      "    2.8177467e-04 -1.2783377e-01]\n",
      "  [ 2.7133652e-03 -3.7767246e-02  4.9384516e-02 ...  9.5747434e-02\n",
      "    7.4668397e-04 -7.8224063e-02]\n",
      "  ...\n",
      "  [ 8.6837165e-02  1.2081293e-01 -1.0253500e-01 ... -6.0062591e-02\n",
      "    1.3146683e-04 -3.0444263e-02]\n",
      "  [ 1.2823616e-01  8.4139861e-02 -9.7969873e-03 ...  1.4082185e-02\n",
      "    3.5240687e-04  3.7147999e-02]\n",
      "  [ 8.5922129e-02  1.0110289e-01 -5.7305817e-02 ...  2.5469355e-02\n",
      "    4.3694774e-05  5.9990045e-02]]], shape=(1, 12, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 入力例\n",
    "text_input = tokenizer('I am a Ironman.', return_tensors='tf', padding='max_length', max_length=15).input_ids\n",
    "visual_input = tf.random.uniform((1, 3, 224, 224), minval=-1, maxval=1)\n",
    "visual_embedding = vit(visual_input).last_hidden_state\n",
    "\n",
    "# テキスト入力をT5エンコーダーで埋め込みに変換\n",
    "text_embedding = t5.encoder(input_ids=text_input).last_hidden_state\n",
    "\n",
    "# デコーダの入力用IDを作成\n",
    "decoder_input_ids = tokenizer('translate English to German: This is a test.', return_tensors='tf').input_ids\n",
    "\n",
    "# VQAモデルを作成\n",
    "model = VQAModel(t5)\n",
    "\n",
    "# 予測\n",
    "output = model([visual_embedding, text_embedding, decoder_input_ids])\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KIBU3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
